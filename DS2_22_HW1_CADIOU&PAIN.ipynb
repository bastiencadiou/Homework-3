{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of used packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data. hw1_devsample and hw1_outofsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10) \n",
    "\n",
    "from functions import *\n",
    "# Load data - set index column, decimal point, separator\n",
    "data = pd.read_csv('hw1_devsample.csv', sep=',',\n",
    "                   decimal='.', index_col='SK_ID_CURR')\n",
    "\n",
    "# print time of data being loaded - use strftime\n",
    "print(f'Data loaded on:   {datetime.datetime.now().strftime(format=\"%Y-%m-%d %H:%M:%S\")}')\n",
    "data_cart = data.copy()\n",
    "\n",
    "data_test = pd.read_csv('hw1_outofsample.csv', sep=',',decimal='.', index_col='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove variables with more than 30% NA.\n",
    "\n",
    "We also remove the time variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_col = []\n",
    "for i in range(0, len(data_cart.isna().sum())):\n",
    "    if data_cart[data_cart.columns[i]].isna().sum()/80000 > 0.3:\n",
    "        del_col.append(data_cart.columns[i])\n",
    "data_cart = data_cart.drop(del_col, axis=1)\n",
    "data_cart = data_cart.drop(['MONTH','TIME','DAY','BASE'], axis=1)\n",
    "\n",
    "data_test = data_test.drop(del_col, axis=1)\n",
    "data_test = data_test.drop(['MONTH','TIME','DAY','BASE'], axis=1)\n",
    "\n",
    "data_cart_fillna = data_cart.copy()\n",
    "data_cart_na = data_cart.copy()\n",
    "\n",
    "data_test_na = data_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to predict on the file on the data from the \"hw1_outofsample.csv\" file, we want our model to train on the same data. Therefore, we want the shape of the two datasets similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cart_na = data_cart_na.dropna(axis = 0)\n",
    "data_cart_na = pd.get_dummies(data_cart_na[data_cart_na.columns])\n",
    "\n",
    "data_test = data_test.dropna(axis = 1)\n",
    "data_test = pd.get_dummies(data_test[data_test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_cart_na.info())\n",
    "print(data_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we store the variable names of the data in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred = list(data_cart_na.columns)\n",
    "cols_test = list(data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find common variables between the train data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_similar = []\n",
    "for i in cols_pred:\n",
    "    for j in cols_test:\n",
    "        if i==j:\n",
    "            col_similar.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_similar=list(set(cols_pred).intersection(cols_test))\n",
    "print(len(col_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only common variables in the data_test.\n",
    "\n",
    "In addition to this, we add the TARGET to the list of variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[col_similar]\n",
    "col_similar_target = col_similar + ['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep the variables and target similar to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cart_na = data_cart_na[col_similar_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first tree is trained using randomly chosen subset of train dataset (we denote data_fraction the fraction of overall train dataset). Similarly, only predictor_fraction fraction of available predictors is used. The remaining trees are then trained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_cart_na['TARGET'].astype(int, copy=False)\n",
    "data_cart_iter = data_cart_na.drop(['TARGET'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree construction criteria :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = 'gini' #(TODO)\n",
    "splitter = 'best' #(TODO)\n",
    "mdepth = 20 #(TODO)\n",
    "min_samples_split = 5 #(TODO)\n",
    "min_samples_leaf = 0.0001 #(TODO)\n",
    "minleaf = 100 #(TODO)\n",
    "random_state = 1\n",
    "max_features = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of our first split data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(data_cart_iter, target, test_size=0.25)\n",
    "print(X_train1.shape[0],X_train1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(\n",
    "    criterion=criterion,\n",
    "    splitter=splitter,\n",
    "    max_depth=mdepth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state,\n",
    "    max_features = max_features\n",
    ")\n",
    "\n",
    "# And fit training data.\n",
    "clf = model.fit(X_train1, Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the yield of this tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict_proba(X_train1)[:,1]\n",
    "pred_test = model.predict_proba(X_test1)[:,1]\n",
    "data_cart_na['Pred'] = 0\n",
    "data_cart_na.loc[list(X_train1.index),'Pred'] = pred_train\n",
    "data_cart_na.loc[list(X_test1.index),'Pred'] = pred_test\n",
    "\n",
    "data_cart_na['Pred_label'] = (data_cart_na['Pred'] >= 0.5).astype(int)\n",
    "\n",
    "data_cart_na['Res'] = abs(data_cart_na['TARGET']-data_cart_na['Pred'])\n",
    "\n",
    "prediction = model.predict_proba(data_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array.\"\"\"\n",
    "    # based on bottom eq: http://www.statsdirect.com/help/content/image/stat0206_wmf.gif\n",
    "    # from: http://www.statsdirect.com/help/default.htm#nonparametric_methods/gini.htm\n",
    "    array = array.flatten() #all values are treated equally, arrays must be 1d\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array) #values cannot be negative\n",
    "    array += 0.0000001 #values cannot be 0\n",
    "    array = np.sort(array) #values must be sorted\n",
    "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
    "    n = array.shape[0]#number of array elements\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_bs = gini(prediction)\n",
    "gini_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a loop to improve the performance of our tree. Iteration by iteration, the weights of the individuals are weighted by the results of the previous tree. This is to improve the AUC.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "n = 10\n",
    "clf = []\n",
    "for n in range(0,n):\n",
    "    \n",
    "    X_train_i, X_test_i, Y_train_i, Y_test_i = train_test_split(data_cart_iter, target, test_size=0.25)\n",
    "\n",
    "    model = tree.DecisionTreeClassifier(\n",
    "    criterion=criterion,\n",
    "    splitter=splitter,\n",
    "    max_depth=mdepth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    max_features = max_features,\n",
    "    random_state=n\n",
    "    )\n",
    "\n",
    "    # And fit training data.\n",
    "    #clf_i = model.fit(X_train_i, Y_train_i, sample_weight = data_cart_na.loc[list(X_train_i.index),'Pred'])\n",
    "    clf_i = model.fit(X_train_i, Y_train_i, sample_weight = gini_bs)\n",
    "\n",
    "    \n",
    "    pred_train = model.predict_proba(X_train_i)[:,1]\n",
    "    pred_test = model.predict_proba(X_test_i)[:,1]\n",
    "    \n",
    "    data_cart_na.loc[list(X_train_i.index),'Pred'] = (pred_train + (n*data_cart_na.loc[list(X_train_i.index),'Pred']))/(n+1)\n",
    "    data_cart_na.loc[list(X_test_i.index),'Pred'] = (pred_test + (n*data_cart_na.loc[list(X_test_i.index),'Pred']))/(n+1)\n",
    "\n",
    "    data_cart_na['Pred_label'] = (data_cart_na['Pred'] >= 0.5).astype(int)\n",
    "    \n",
    "    data_cart_na['Res'] = abs(data_cart_na['TARGET'] - data_cart_na['Pred'])\n",
    "    \n",
    "    print('*************** Evaluation After Iteration '+str(n+1)+' *********')\n",
    "    \n",
    "    #score_te = model.score(X_test_i, Y_test_i)\n",
    "    #print('Accuracy Score: ', score_te)\n",
    "    \n",
    "    #pred_labels_test = model.predict_proba(X_test_i) \n",
    "    #print('Taux derreur', 1.0 - metrics.accuracy_score(Y_test_i,pred_labels_test))\n",
    "\n",
    "    prediction = (model.predict_proba(data_test)[:,1] + (n*prediction))/(n+1)\n",
    "\n",
    "    gini_bs = gini(prediction)\n",
    "    print('Gini' , gini_bs)\n",
    "    print('AUC',roc_auc_score(Y_test_i, data_cart_na.loc[list(Y_test_i.index),'Pred'])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score that our algorithm gives us is very good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting target values for the test data with our forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our results in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saving = pd.DataFrame({'SK_ID_CURR' : data_test.index ,'prediction' : prediction})\n",
    "data_saving.to_csv('DS2_22_HW1_CADIOU&PAIN.csv',index=False,sep=',')\n",
    "data_saving.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we created a copy of data_cart namd data_cart_fillna. It was created to propose a different version of handling missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we do exactly the same as before but by giving a median value to the NA's. However, we obtain results that seems to be not much interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cart_fillna = pd.get_dummies(data_cart_fillna[data_cart_fillna.columns])\n",
    "data_cart_fillna= data_cart_fillna.fillna(data_cart_fillna.median())\n",
    "data_test_na = pd.get_dummies(data_test_na[data_test_na.columns])\n",
    "data_test_na = data_test_na.fillna(data_test_na.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_pred_na = list(data_cart_fillna.columns)\n",
    "cols_test_na = list(data_test_na.columns)\n",
    "col_similar_na = []\n",
    "for i in cols_pred_na:\n",
    "    for j in cols_test_na:\n",
    "        if i==j:\n",
    "            col_similar_na.append(i)\n",
    "col_similar_na=list(set(cols_pred_na).intersection(cols_test_na))\n",
    "print(len(col_similar_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_na = data_test_na[col_similar_na]\n",
    "col_similar_target_na = col_similar_na + ['TARGET']\n",
    "data_cart_fillna = data_cart_fillna[col_similar_target_na]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_na = data_cart_fillna['TARGET'].astype(int, copy=False)\n",
    "data_cart_iter_fillna = data_cart_fillna.drop(['TARGET'], axis = 1)\n",
    "\n",
    "criterion = 'gini' #(TODO)\n",
    "splitter = 'best' #(TODO)\n",
    "mdepth = 20 #(TODO)\n",
    "min_samples_split = 5 #(TODO)\n",
    "min_samples_leaf = 0.0001 #(TODO)\n",
    "minleaf = 100 #(TODO)\n",
    "random_state = 1\n",
    "max_features = 0.8\n",
    "\n",
    "X_train1_na, X_test1_na, Y_train1_na, Y_test1_na = train_test_split(data_cart_iter_fillna, target_na, test_size=0.25)\n",
    "print(X_train1_na.shape[0],X_train1_na.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_na = tree.DecisionTreeClassifier(\n",
    "    criterion=criterion,\n",
    "    splitter=splitter,\n",
    "    max_depth=20,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state,\n",
    "    max_features = max_features\n",
    ")\n",
    "\n",
    "# And fit training data.\n",
    "clf_na = model_na.fit(X_train1_na, Y_train1_na)\n",
    "\n",
    "pred_train_na = model_na.predict_proba(X_train1_na)[:,1]\n",
    "pred_test_na = model_na.predict_proba(X_test1_na)[:,1]\n",
    "data_cart_fillna['Pred'] = 0\n",
    "data_cart_fillna.loc[list(X_train1_na.index),'Pred'] = pred_train_na\n",
    "data_cart_fillna.loc[list(X_test1_na.index),'Pred'] = pred_test_na\n",
    "\n",
    "data_cart_fillna['Pred_label'] = (data_cart_fillna['Pred'] >= 0.5).astype(int)\n",
    "\n",
    "data_cart_fillna['Res'] = abs(data_cart_fillna['TARGET']-data_cart_fillna['Pred'])\n",
    "\n",
    "prediction_na = model_na.predict_proba(data_test_na)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "n = 10\n",
    "clf_na = []\n",
    "for n in range(0,n):\n",
    "    \n",
    "    X_train_i, X_test_i, Y_train_i, Y_test_i = train_test_split(data_cart_iter_fillna, target_na, test_size=0.25)\n",
    "\n",
    "    model = tree.DecisionTreeClassifier(\n",
    "    criterion=criterion,\n",
    "    splitter=splitter,\n",
    "    max_depth=20,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    max_features = max_features,\n",
    "    random_state=n\n",
    "    )\n",
    "\n",
    "    # And fit training data.\n",
    "    clf_i = model.fit(X_train_i, Y_train_i, sample_weight = data_cart_fillna.loc[list(X_train_i.index),'Pred'])\n",
    "    \n",
    "    pred_train = model.predict_proba(X_train_i)[:,1]\n",
    "    pred_test = model.predict_proba(X_test_i)[:,1]\n",
    "    \n",
    "    data_cart_fillna.loc[list(X_train_i.index),'Pred'] = (pred_train + (n*data_cart_fillna.loc[list(X_train_i.index),'Pred']))/(n+1)\n",
    "    data_cart_fillna.loc[list(X_test_i.index),'Pred'] = (pred_test + (n*data_cart_fillna.loc[list(X_test_i.index),'Pred']))/(n+1)\n",
    "\n",
    "    data_cart_fillna['Pred_label'] = (data_cart_fillna['Pred'] >= 0.5).astype(int)\n",
    "    \n",
    "    data_cart_fillna['Res'] = abs(data_cart_fillna['TARGET'] - data_cart_fillna['Pred'])\n",
    "    \n",
    "    print('*************** Evaluation After Iteration '+str(n+1)+' *********')\n",
    "    \n",
    "    prediction = (model.predict_proba(data_test_na)[:,1] + (n*prediction))/(n+1)\n",
    "\n",
    "\n",
    "    print('AUC',roc_auc_score(Y_test_i, data_cart_fillna.loc[list(Y_test_i.index),'Pred_label'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(data_test_na)\n",
    "data_saving = pd.DataFrame({'SK_ID_CURR' : data_test_na.index ,'prediction' : y_pred_proba})\n",
    "data_saving.to_csv('DS2_22_HW1_CADIOU&PAIN.csv',index=False,sep=',')\n",
    "data_saving.info()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1275a05ae34f0c315803f9d1758b76ed30a4d7265e5e486823be65aff6a6df27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
